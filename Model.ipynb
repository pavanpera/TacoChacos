{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOVOsE6uclBKqWu4dstFmqt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ct0Fva9xaL7i"},"source":["**Importing Libraries**"]},{"cell_type":"code","metadata":{"id":"yVy-n4M9aKR1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632082473593,"user_tz":-330,"elapsed":1159,"user":{"displayName":"Pavan Pera (LatentView)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13019787790005327912"}},"outputId":"0c5c8f8a-6f62-4462-ec78-64583c19ee49"},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer=WordNetLemmatizer()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","metadata":{"id":"Xc-1R0zZaZCJ"},"source":["**Importing Scraped Movies Data**"]},{"cell_type":"code","metadata":{"id":"WW1cwqZvadK4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632082475934,"user_tz":-330,"elapsed":912,"user":{"displayName":"Pavan Pera (LatentView)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13019787790005327912"}},"outputId":"52a7c101-bd65-4113-8956-db06fa612c61"},"source":["movies_data=pd.read_csv(r\"/content/movies_final.csv\")\n","movies_data.info()\n","df = movies_data[['movie','genres','director','stars','plot']]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 86655 entries, 0 to 86654\n","Data columns (total 10 columns):\n"," #   Column         Non-Null Count  Dtype  \n","---  ------         --------------  -----  \n"," 0   Unnamed: 0     86655 non-null  int64  \n"," 1   movie          86655 non-null  object \n"," 2   year           86655 non-null  int64  \n"," 3   time_minute    86655 non-null  object \n"," 4   imdb_rating    86655 non-null  float64\n"," 5   genres         86655 non-null  object \n"," 6   plot           86655 non-null  object \n"," 7   director       86655 non-null  object \n"," 8   stars          86655 non-null  object \n"," 9   primary_genre  86655 non-null  object \n","dtypes: float64(1), int64(2), object(7)\n","memory usage: 6.6+ MB\n"]}]},{"cell_type":"markdown","metadata":{"id":"pglR1UO0aoTt"},"source":["**Text Pre-Processing**"]},{"cell_type":"code","metadata":{"id":"UfcqLTyOakQN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632082478010,"user_tz":-330,"elapsed":7,"user":{"displayName":"Pavan Pera (LatentView)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13019787790005327912"}},"outputId":"31be5746-d180-4bd9-d1dc-5a23dd4caf37"},"source":["df['genres'] = df['genres'].map(lambda x: x.lower().split(','))\n","split_df = pd.DataFrame(df['genres'].tolist(), columns=['genre1', 'genre2', 'genre3'])\n","df = pd.concat([df, split_df], axis=1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"Entry point for launching an IPython kernel.\n"]}]},{"cell_type":"code","metadata":{"id":"xtpk-QXba4te"},"source":["df['stars'] = df['stars'].map(lambda x: x.lower().split(','))\n","split_df = pd.DataFrame(df['stars'].tolist(), columns=['star1', 'star2', 'star3','star4','star5'])\n","df = pd.concat([df, split_df], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BSI0QJOnbIl-"},"source":["# changing case\n","df['plot'] = df['plot'].map(lambda x: x.lower())\n","df['movie']=df['movie'].map(lambda x: x.lower())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KTRyGzPgbNle"},"source":["#merging together first and last name for each actor \n","for index, row in df.iterrows():\n","    if row['star1'] is not None:\n","     row['star1'] = row['star1'].lower().replace(' ','')\n","    if row['star2'] is not None:\n","     row['star2'] = row['star2'].lower().replace(' ','')\n","    if row['star3'] is not None:\n","     row['star3'] = row['star3'].lower().replace(' ','')\n","    if row['star4'] is not None:\n","     row['star4'] = row['star4'].lower().replace(' ','')\n","    if row['star5'] is not None:\n","     row['star5'] = row['star5'].lower().replace(' ','')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r0qfxoKWXvqL"},"source":["df=df.drop(['genres','stars'],axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"igZNvkG2l85Z"},"source":["df=df.drop_duplicates(subset=['movie'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AnLG1jw5bgfV"},"source":["# setting movie name as index\n","df.set_index('movie', inplace = True) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ggTqeZebmtn"},"source":["#seperating each word from each row and column and merging as list\n","bag_of_words=[]\n","for index, row in df.iterrows():\n","    words = []\n","    for col in df.columns:\n","            word=nltk.word_tokenize(str(row[col]))\n","            words.extend(word)\n","    bag_of_words.append(words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L13f201Kb5Qu"},"source":["#Adding all keywords as column to dataframe\n","df['bag_of_words']=bag_of_words  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xfE1k2B0cAhs"},"source":["#Dropping old columns\n","df1=df.drop(['director', 'plot', 'genre1', 'genre2', 'genre3', 'star1', 'star2',\n","       'star3', 'star4','star5'],axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l_5IEuFOXyut"},"source":["ignore_letters=[',','.','[',']','!','\\'s']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x2NcioZxcG49"},"source":["for index, row in df1.iterrows():\n","   for col in df1.columns:\n","       row[col]=[lemmatizer.lemmatize(w.lower()) for w in row[col] if w not in ignore_letters]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JkOS4T8-cPd_"},"source":["#making corpus for each movie\n","for index, row in df1.iterrows():\n","   for col in df1.columns:\n","       str1=\" \"\n","       str1=str1.join(row[col])\n","       row[col]=str1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U370e2VPcVYV","colab":{"base_uri":"https://localhost:8080/","height":111},"executionInfo":{"status":"ok","timestamp":1632082584331,"user_tz":-330,"elapsed":563,"user":{"displayName":"Pavan Pera (LatentView)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13019787790005327912"}},"outputId":"6c3b09d9-bdf1-4424-dd82-6369a1e1ed9b"},"source":["#sample for first movie\n","df1.iloc[0:1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>bag_of_words</th>\n","    </tr>\n","    <tr>\n","      <th>movie</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>shang-chi and the legend of the ten rings</th>\n","      <td>destin daniel cretton shang-chi the master of ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                bag_of_words\n","movie                                                                                       \n","shang-chi and the legend of the ten rings  destin daniel cretton shang-chi the master of ..."]},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"S1Tb8BRdsPtO"},"source":["df2=df1.iloc[0:15000]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RbADpp3QcuGe"},"source":["**Text Vectorization**"]},{"cell_type":"code","metadata":{"id":"HJncHO_tcr6W"},"source":["count = CountVectorizer()\n","count_matrix = count.fit_transform(df2['bag_of_words'])\n","indices = pd.Series(df2.index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8x7GNqJtc6iA"},"source":["# now storing vector matrix and movie titles as pickle\n","import pickle\n","pickle.dump(count_matrix, open(\"count_matrix.pickel\", \"wb\"))\n","pickle.dump(indices, open(\"indices.pickel\", \"wb\"))"],"execution_count":null,"outputs":[]}]}